{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Applied Data Science (MAST30034) Tutorial 1"
   ],
   "metadata": {
    "heading_collapsed": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "- Basic intro to Apache Spark (30-60 minutes)\n",
    "- Project 1 Tips (yes, it's already out and we **strongly recommend you start today**) (remainder of time)\n",
    "_________________"
   ],
   "metadata": {
    "hidden": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Starting a Spark Session\n",
    "To begin with Spark, we need to start a `SparkSession` class.\n",
    "- `appName`: Name of the Spark app\n",
    "- `config`: Configurations to initialise with. We will initialise this example with `'spark.sql.repl.eagerEval.enabled'` which enables a nicer HTML display (similar to `pandas`) for the DataFrame outputs.\n",
    "- `.getOrCreate()`: Create the spark session."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a spark session (which will run spark jobs)\n",
    "spark = (\n",
    "    SparkSession.builder.appName(\"MAST30034 Tutorial 1\")\n",
    "    .config(\"spark.sql.repl.eagerEval.enabled\", True) \n",
    "    .config(\"spark.sql.parquet.cacheMetadata\", \"true\")\n",
    "    .config(\"spark.sql.session.timeZone\", \"Etc/UTC\")\n",
    "    .getOrCreate()\n",
    ")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "24/08/01 23:11:18 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "A general note is to understand that Spark is **immutable**. We'll discuss it further down the track, but for now, just remember this!\n",
    "\n",
    "Documentation is also going to be your saving grace. If you have tried your best **and have read the documentation and researched on Stack Overflow** but still can't get it working, then try Chat-GPT. If that's not possible, then you can ask your tutor for help :P\n",
    "\n",
    "## Reading in the Parquet\n",
    "As of 2022, TLC has made a **great decision** to drop `csv` and adopt `parquet` formats instead. So, what's a `parquet`? \n",
    "\n",
    "Related materials:\n",
    "1. [What if you could get the simplicity, convenience, interoperability, and storage niceties of an old-fashioned CSV with the speed of a NoSQL database and the storage requirements of a gzipped file? Enter Parquet.](https://databricks.com/session/spark-parquet-in-depth)\n",
    "2. [The Parquet Format and Performance Optimization Opportunities](https://databricks.com/session_eu19/the-parquet-format-and-performance-optimization-opportunities)\n",
    "\n",
    "CSV:\n",
    "- `csv` are tabular data formats read in line by line using a `,` delimiter.\n",
    "- That is, these are stored by rows.\n",
    "- They consume a lot of disk space and are one of the **most inefficient** ways of storing data.\n",
    "- However, they are widely used and easy to use for smaller datasets.\n",
    "\n",
    "Parquet:\n",
    "- `parquet` on the other hand is stored in columns and (ELI5) are very efficient with data formats.\n",
    "- For example, a single row in a `csv` can contain several different data types. \n",
    "- `parquet` just have the single data type per column, allowing compression algorithms to be applied to reduce disk space and read efficiency.\n",
    "- For alternatives to `csv` for row based data formats, you can take a look at `avro`.\n",
    "\n",
    "![Divisions of storage format](../../media/storageformat.png)\n",
    "\n",
    "Cost Analysis from Amazon Web Services (AWS): ![image.png](https://miro.medium.com/max/1400/1*vdasMxTjInhBXIRA8K1XYQ.png)\n",
    "\n",
    "Spark Docs\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrameReader.parquet.html?highlight=read%20parquet\n",
    "- https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html?highlight=show#pyspark.sql.DataFrame.show"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# sdf = spark df = spark data frame\n",
    "sdf = spark.read.parquet('../../data/tlc_data/2024-01.parquet')\n",
    "sdf.show(1, vertical=True, truncate=100)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: file:/Users/thangnguyen/Documents/GitHub/MAST30034_Python/data/tlc_data/2024-01.parquet.",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# sdf = spark df = spark data frame\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m sdf \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../../data/tlc_data/2024-01.parquet\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m sdf\u001b[38;5;241m.\u001b[39mshow(\u001b[38;5;241m1\u001b[39m, vertical\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: file:/Users/thangnguyen/Documents/GitHub/MAST30034_Python/data/tlc_data/2024-01.parquet."
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:02:52.129150Z",
     "start_time": "2023-07-24T04:02:45.036746Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The Spark UI is quite ugly at times, so if you miss `pandas` and want the \"nice\" display you can set `spark.sql.repl.eagerEval.enabled` to `True` in the config. To see the nice format, use `.limit()`.\n",
    "\n",
    "`pyspark`'s `.show()`, `.head()`, `.limit()`, etc are all alternatives to `pandas`'s `.head()` (`.tail` exists in both `pandas` and `pyspark`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "sdf.limit(5)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'sdf' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msdf\u001b[49m\u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m5\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sdf' is not defined"
     ]
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:02.224352Z",
     "start_time": "2023-07-24T04:02:59.918648Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Spark has also been designed to read in directories as well! We won't be using it for the tutorial, but if you wish to use it for your project, feel free to do so!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "# here, we give it the directory, rather than a specific parquet\n",
    "sdf_all = spark.read.parquet('../../data/tlc_data/')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:02.413366Z",
     "start_time": "2023-07-24T04:03:02.225507Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To count the number of records, use the `.count()` method. The equivalent in `pandas` would be `len(df)` or `df.shape` or alternative. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "sdf.count(), sdf_all.count()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(2964624, 9554778)"
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:05.617652Z",
     "start_time": "2023-07-24T04:03:04.487043Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To view the data types of our `sdf`, we can use:\n",
    "- `.printSchema()` to print it nicely.\n",
    "- `.schema` for the actual schema object\n",
    "\n",
    "The `pandas` alternative is `df.dtypes`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sdf.printSchema()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:05.638872Z",
     "start_time": "2023-07-24T04:03:05.619077Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sdf.schema"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:05.731117Z",
     "start_time": "2023-07-24T04:03:05.723079Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "See here for the available data types: https://spark.apache.org/docs/latest/sql-ref-datatypes.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Basic Operations\n",
    "### Selection\n",
    "To show a specific column, we will use `sdf.select(col).limit(5)`. \n",
    "- The equivalent in `pandas` is `df[col].head()`.\n",
    "\n",
    "To _access_ a specific column, use the `sdf[col]` syntax (equivalent to `df[col]`). Avoid using `sdf.col` or `df.col` as it is **not** robust (cannot handle columns with spaces) or future-proof. \n",
    "\n",
    "For multiple columns, pass them through an array as usual.\n",
    "\n",
    "Please note, this selection is only good for seeing bits and pieces of data and not for filtering."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sdf.select('passenger_count').limit(5)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:09.238597Z",
     "start_time": "2023-07-24T04:03:08.260421Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Students to write code to select the first 10 records for `passenger_count` and `trip_distance`_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# write code here to select the first 10 records for `passenger_count` and `trip_distance`"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:13.527228Z",
     "start_time": "2023-07-24T04:03:12.397628Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Filtering\n",
    "For filtering data, we use `sdf.filter(condition)` or `sdf.where(condition)` (they are aliases of each other)\n",
    "- The equivalent in `pandas` is `df.loc[condition].head()`\n",
    "- When using multiple conditions, use parenthesis and `&` (AND) / `|` (OR)\n",
    "\n",
    "To do so, we will use `pyspark.sql.functions.col` to specify the column we are working with."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "from pyspark.sql import functions as F"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:15.415531Z",
     "start_time": "2023-07-24T04:03:15.391510Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "F.col(\"passenger_count\")"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:15.932836Z",
     "start_time": "2023-07-24T04:03:15.595811Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As you can see, this is just a \"column type\" and doesn't do much. We'll come back to this in the next tutorial. For now, take our word."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sdf.filter(F.col('passenger_count') == 5).limit(5)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:18.276168Z",
     "start_time": "2023-07-24T04:03:16.125585Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "_Students to write code to retrieve all non-zero passenger counts and all non-zero trip distances using `.filter()`_"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "source": [
    "# write code here to retrieve all non-zero passenger counts and all non-zero trip distances using filter()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:19.219000Z",
     "start_time": "2023-07-24T04:03:18.277699Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### GroupBy (Aggregation)\n",
    "To groupby the data (i.e mean), we can use `sdf.groupby(col).mean(aggregated columns).limit(5)`\n",
    "- The equivalent in `pandas` is `df.groupby(col)[aggregated columns].mean().head()`"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sdf.groupby('passenger_count').mean('trip_distance').limit(5)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:23.940677Z",
     "start_time": "2023-07-24T04:03:21.475197Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can also apply multiple different aggregations and change their output names using `.agg()` and `.alias()`! To see the list of all SQL functions, visit https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/functions.html\n",
    "\n",
    "We'll also use `.orderBy()` to display the results nicely."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "aggregated_results = sdf \\\n",
    "                    .groupBy(\"passenger_count\") \\\n",
    "                    .agg(\n",
    "                        F.mean(\"total_amount\").alias(\"avg_trip_amount_usd\"),\n",
    "                        F.max(\"trip_distance\").alias(\"max_trip_distance_miles\")\n",
    "                    ) \\\n",
    "                    .orderBy(\"passenger_count\")\n",
    "\n",
    "aggregated_results.show()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:25.388514Z",
     "start_time": "2023-07-24T04:03:23.942281Z"
    },
    "code_folding": []
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Saving Data\n",
    "By default, Spark will save your data sources as a `parquet` (highly recommended). If you wish to take a smaller sample and save it as a `csv` to load into `pandas`, that is also fine."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "aggregated_results.write.mode('overwrite').parquet('../../data/tute_data/aggregated_results')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:27.419466Z",
     "start_time": "2023-07-24T04:03:25.389795Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Your directory may look a bit funky like this:\n",
    "\n",
    "![image.png](../../media/aggregated_results_dir.png)\n",
    "\n",
    "Don't worry, just leave it as is (we don't have time to cover everything about Spark unfortunately) and you can just read in the directory as is."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "temp_results = spark.read.parquet('../../data/tute_data/aggregated_results')\n",
    "temp_results.show()"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:27.775017Z",
     "start_time": "2023-07-24T04:03:27.420416Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary (and Break)\n",
    "Cool, we've covered the very very basics of Spark and will now cover the basics of plotting.\n",
    "\n",
    "Rest assured, we will cover more intricate transformations for the next tutorial (which you may go ahead in of course)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "---"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sampling Data for Plotting"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Whilst Spark is amazing at handling big data sets, it isn't a great idea to plot all of it. We suggest taking a maximum of 5% of records for the tutorial. \n",
    "\n",
    "You can up it to your requirements, but we recommend sticking to less than 1 million records per month for visualization purposes.\n",
    "\n",
    "**Project 1 Checklist:**\n",
    "- You have justified your sample size (i.e due to runtime, distribution of data, etc)\n",
    "- You have justified your sampling method (i.e random, stratified, etc)\n",
    "- You have detailed in your report that you have sampled for visualization purposes BUT your analysis still uses the full distribution of data\n",
    "- You mention any issues that can potentially be caused by sampling (i.e biased visualisation if using random)\n",
    "\n",
    "Remember, it is your responsibility as the student (future Data Scientist) to convince the tutor (your stakeholders) that your justifications and assumptions are correct!"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "To sample your data and convert it into a `pandas` dataframe, you can use the `.toPandas()` and save a sample of the `sdf` to read it in. We will also fix the random seed to be `0` just for consistency."
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-12T05:45:28.804443Z",
     "start_time": "2022-07-12T05:45:28.724408Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "SAMPLE_SIZE = 0.05"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:28.560339Z",
     "start_time": "2023-07-24T04:03:28.545932Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "df = sdf.sample(SAMPLE_SIZE, seed=0).toPandas()\n",
    "df.to_csv('../../data/tute_data/sample_data.csv', index=False)"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:34.788025Z",
     "start_time": "2023-07-24T04:03:28.760727Z"
    },
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "df.to_parquet('../../data/tute_data/sample_data.parquet')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:34.855142Z",
     "start_time": "2023-07-24T04:03:34.789165Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Just spend a moment and look at the disk space the `csv` takes for the 5% sample size (16.1mb). Compare that to the `parquet` which isn't even 4mb, let alone the full sample size in `parquet` format taking only 48mb of disk space.\n",
    "\n",
    "Let that sink in and give our thanks to the devs who made Spark. "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "df_csv = pd.read_csv('../../data/tute_data/sample_data.csv')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:35.029098Z",
     "start_time": "2023-07-24T04:03:34.856046Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "%%time\n",
    "df_parquet = pd.read_parquet('../../data/tute_data/sample_data.parquet')"
   ],
   "outputs": [],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-24T04:03:35.040185Z",
     "start_time": "2023-07-24T04:03:35.030786Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We recommend you save every dataframe or aggregation as `parquet` so you don't keep running your notebook from top to bottom waiting 20 years for a result, or have so many variables and dataframes defined that you run out of memory for small transformations.\n",
    "\n",
    "We strongly suggest you have a `code` folder in your Project 1 directory with the following structure:\n",
    "- `preprocessing_notebook_part_1.ipynb`: outputs a structured parquet format and saves it.\n",
    "- `preprocessing_notebook_part_2.ipynb`: reads in the output from above and does some aggregations and sampling before saving it.\n",
    "- `data_analysis_xyz.ipynb`: conducts analysis on a single sample or aggregation from the output above.\n",
    "- `data_analysis_abc.ipynb`: conducts analysis on another single sample or aggregation from the output above.\n",
    "- `...`\n",
    "\n",
    "This is a very basic version of what you call a \"data pipeline\" (or ETL pipeline, etc)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_________________\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Project 1 Tips and Questions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### IMPORTANT PLEASE READ THIS\n",
    "First and foremost, you want to be familiar with the homepage https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\n",
    "\n",
    "Read through the relevant data dictionaries:\n",
    "- **MUST READ:** https://www1.nyc.gov/assets/tlc/downloads/pdf/trip_record_user_guide.pdf\n",
    "- https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_yellow.pdf\n",
    "- https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_green.pdf\n",
    "- https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_fhv.pdf\n",
    "- https://www1.nyc.gov/assets/tlc/downloads/pdf/data_dictionary_trip_records_hvfhs.pdf\n",
    "\n",
    "Why? Your tutors can be treated as \"experts\" in this field. To prepare you for the Industry Project, we need to assess students on adhering to requirements and business rules. \n",
    "\n",
    "The tutor team knows this dataset inside out. If you are incorrectly filtering records without sufficient justification, you will be losing marks as per requirements.\n",
    "\n",
    "### An Incorrect Example\n",
    "- Scenario: Student does analysis on `tip_amount` and finds several `NULL` values and either drops them or includes it in the analysis. Later on, they use a regression model to predict this value.\n",
    "\n",
    "- Result: According to the data dictionary, `tip_amount` is automatically populated for credit card tips (`payment_type` is `1`). Cash tips are not included. This means that the students' analysis included all payment types despite this field clearly specifying the rule. \n",
    "\n",
    "- Penalty: The student will lose marks on the analysis section. The modelling section will be marked _assuming_ they got this filtering method correct. However, if another issue pops up due to this, there will be another penalty applied. Please get this right!\n",
    "\n",
    "- Solution: Student should filter for only `payment_type=1` and now, the student can (hopefully) conduct correct analysis on `tip_amount`.\n",
    "\n",
    "Several students over the past few years have lost many marks for simple rules like this (especially `tip_amount`)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Readable Code\n",
    "- We will be assessing the quality of your code and how you present it in your notebooks. \n",
    "- This is because there is no point writing code that cannot be easily interpreted. At the end of the day, employers and clients are not only paying for your analysis, but also the corresponding code. \n",
    "- If your code is confusing or difficult to read, there is little chance your client will come back to you.\n",
    "\n",
    "**Variable Names:**  \n",
    "As long as you are consistent, then it is fine. For example, commit to either using:\n",
    "- Snake Case: words are seperated by underscores such as `variable_name`\n",
    "- Camel Case: words are seperated by captials such as `variableName`\n",
    "\n",
    "Your variables should be contextual and describe the code. That is, try to name your variables to be understandable **without comments**.\n",
    "\n",
    "**Comments and Docstrings (w.r.t JupyterNotebook Cells):**  \n",
    "Cells in Jupyter Notebook should aim to do one \"block of logic\" at a time (i.e importing libraries, defining functions, filtering rows, etc).\n",
    "- If it takes a reader more than a few seconds to understand your cell, you need comments.\n",
    "- Your functions need to have docstrings describing what they do. If you forgot, search it online or go visit your COMP10001 Grok course.\n",
    "- Use markdown cells for longer comments or explaining logic, inline comments in code for short descriptions of hard-to-understand code.\n",
    "\n",
    "We won't ask you to run `flake8` or `pylint` on your notebooks. We just ask for good comments in the code and markdown cells, reasonable variable names, and clean directories.\n",
    "\n",
    "Here is a good example of good docstring + comments for functions.\n",
    "\n",
    "```python\n",
    "def some_function(some_val: str) -> str:\n",
    "    \"\"\"\n",
    "    This function takes in some string value\n",
    "    and outputs some string value via some transformation.\n",
    "    \"\"\"\n",
    "    # make sure the casing is correct\n",
    "    new_val = some_val.casefold()\n",
    "    return new_val\n",
    "```"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.11.2 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "1c6f7b18ea35922dad1f927d5d0123541ee5478d7a9729c6a2c6ed680be427a0"
   }
  },
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}